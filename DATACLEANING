#Para criar um modelo de predição de palavras você deve:

#I-) Selecionar o diretório e baixar os arquivos:

setwd("C:/Users/smsanda/Documents/en_US")

newsData <- readLines(file("en_US.news.txt"))
blogData <- readLines(file("en_US.blogs.txt"))
twitterData <- readLines(file("en_US.twitter.txt"))

print(paste("News Data Length = ", length(newsData),
            ", News Blog Length = ", length(blogData),
            ", News twitter Length = ", length(twitterData)
            ))
            
#II-) Load the required libraries for processing of the datas.
require(tm)
require(SnowballC)
require(RWeka)
require(data.table)
require(stringr)
library(tidytext)
library(dplyr)
library(data.table)            

#################################################################################################################################

#III-) Select a sample from the texts files
library(tm)
## Loading required package: NLP
#Load 5000 lines from every set in corpus
merged <- paste(newsData[1:5000], blogData[1:5000], twitterData[1:5000])
corpus <- VCorpus(VectorSource(merged))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, detectNonAsciiChar)
corpus <- tm_map(corpus, removeNonAsciiWord)
corpus <- tm_map(corpus, removeHTTPS)
corpus <- tm_map(corpus, removeHTTP)
corpus <- tm_map(corpus, removeFTP)
corpus <- tm_map(corpus, removeWWW)
corpus <- tm_map(corpus, removeHashTag)
corpus <- tm_map(corpus, removeTwitterRT)
corpus <- tm_map(corpus, removeCharRepetition)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords,bad_words)

write.table(corpus, file="C://Users/smsanda/Documents/en_US"/corpus.txt", row.names = F)

#####Create a corpus#########################################################################
corpus<-VCorpus(DirSource(directory="C:\\Users/smsanda/Documents/en_US"\\Samples",encoding="UTF-8"),readerControl = list(language="us"))

####Preparing a tidy text document
tidy_docs<-tidy(corpus)
###Generating frquencies
############uni grams##################################################
token_uni<-tidy_docs %>% unnest_tokens(words,text)

freq_uni<-token_uni %>% count(words, sort=TRUE)
uiL<-str_split(freq_uni$words," ")
freq_uni$start<-sapply(uiL,FUN=function(x) x[1])
freq_uni$end<-sapply(uiL,FUN=function(x) x[1])

rm(token_uni, uiL)


######################bi grams#####################################################################################
token_bi<-tidy_docs %>% unnest_tokens(bigram,text,token="ngrams", n=2)

freq_bi<-token_bi %>% count(bigram, sort=TRUE)
biL<-str_split(freq_bi$bigram," ")
freq_bi$start<-sapply(biL,FUN=function(x) x[1])
freq_bi$end<-sapply(biL,FUN=function(x) x[2])

rm(token_bi, biL)

######################tri grams#####################################################################################
token_tri<-tidy_docs %>% unnest_tokens(trigram,text,token="ngrams", n=3)

freq_tri<-token_tri %>% count(trigram, sort=TRUE)
triL<-str_split(freq_tri$trigram," ")
freq_tri$start<-sapply(triL,FUN=function(x) paste(x[1],x[2]))
freq_tri$end<-sapply(triL,FUN=function(x) x[3])

rm(token_tri, triL)

######################quadra grams#####################################################################################
token_quadi<-tidy_docs %>% unnest_tokens(quadragram,text,token="ngrams", n=4)

freq_quadi<-token_quadi %>% count(quadragram, sort=TRUE)
quadiL<-str_split(freq_quadi$quadragram," ")
freq_quadi$start<-sapply(quadiL,FUN=function(x) paste(x[1],x[2],x[3]))
freq_quadi$end<-sapply(quadiL,FUN=function(x) x[4])

rm(token_quadi, quadiL)

######################penta grams#####################################################################################
token_penta<-tidy_docs %>% unnest_tokens(pentagram,text,token="ngrams", n=5)

freq_penta<-token_penta %>% count(pentagram, sort=TRUE)
pentaL<-str_split(freq_penta$pentagram," ")
freq_penta$start<-sapply(pentaL,FUN=function(x) paste(x[1],x[2],x[3],x[4]))
freq_penta$end<-sapply(pentaL,FUN=function(x) x[5])

rm(token_penta, pentaL)
rm(corpus)

########################################################################################################################
##Clean up lower frequency terms to reduce size of datasets####################################################################################
freq_bi<-subset(freq_bi, freq_bi$n>=2)
freq_tri<-subset(freq_tri, freq_tri$n>=2)
freq_quadi<-subset(freq_quadi, freq_quadi$n>=2)
freq_penta<-subset(freq_penta, freq_penta$n>=2)

########################Write the frequency datasets to file; to be used later for app upload#########################

write.csv(freq_uni, file="unigrams.csv", row.names = F)
write.csv(freq_bi, file="bigrams.csv", row.names = F)
write.csv(freq_tri, file="trigrams.csv", row.names = F)
write.csv(freq_quadi, file="quadragrams.csv", row.names = F)
write.csv(freq_penta, file="pentagrams.csv", row.names = F)
