#This script defines the preparation of uni, bi, tri, quadra and pentagram datasets from a sample
# of a large corpus. The corpus is prepared using a large input text files containing samples from twitter, news and blogs
# the script describes downloading and copying the files and cleaning the database and constructing a corpus. 
# the corpus is then cleaned and broken down to sonstruct uni, bi, tri, quadra, and pentagrams and their frequencies

setwd("C:/Users/smsanda/Documents/en_US")

news <- readLines(file("en_US.news.txt"))
blogs<- readLines(file("en_US.blogs.txt"))
twit <- readLines(file("en_US.twitter.txt"))

print(paste("News Data Length = ", length(news),
            ", News Blog Length = ", length(blogs),
            ", News twitter Length = ", length(twit)
            ))
            
#Load the required libraries
require(tm)
require(SnowballC)
require(RWeka)
require(data.table)
require(stringr)
library(tidytext)
library(dplyr)
library(data.table)

##A function to sample lines from a text file
sample_files <- function(x) {
    x <- sample(x, length(x)*0.1)# 10% of the lines are sampled
}

####Functions to remove special characters
detectNonAsciiChar <- function(x) iconv(x, from="UTF-8", to="ASCII",sub="X")##detects a nonASCII character and substitutesit with "X"
removeNonAsciiWord <- function(x) gsub("[a-z]*X+[a-z]*", " ", x)##removes the detected nonASCII characters
removeHTTPS <- function(x) gsub("https://(.*)[.][a-z]+|https://[a-z]+", " ", x)
removeHTTP  <- function(x) gsub("http://(.*)[.][a-z]+|https://[a-z]+", " ", x)
removeFTP <- function(x) gsub("ftp://(.*)[.][a-z]+|https://[a-z]+", " ", x)
removeWWW <- function(x) gsub("www(.*)[.][a-z]+|www.", " ", x)
removeHashTag <- function(x) gsub("#[a-z0-9]+", " ", x)
removeTwitterRT <- function(x) gsub("^rt |^rt:", " ", x)
removeCharRepetition <- function(x) {
    a <- gsub("[a-z]*aaa[a-z]*", " ", x)
    a <- gsub("[a-z]*bbb[a-z]*", " ", a)
    a <- gsub("[a-z]*ccc[a-z]*", " ", a)
    a <- gsub("[a-z]*ddd[a-z]*", " ", a)
    a <- gsub("[a-z]*eee[a-z]*", " ", a)
    a <- gsub("[a-z]*fff[a-z]*", " ", a)
    a <- gsub("[a-z]*ggg[a-z]*", " ", a)
    a <- gsub("[a-z]*hhh[a-z]*", " ", a)
    a <- gsub("[a-z]*iii[a-z]*", " ", a)
    a <- gsub("[a-z]*jjj[a-z]*", " ", a)
    a <- gsub("[a-z]*kkk[a-z]*", " ", a)
    a <- gsub("[a-z]*lll[a-z]*", " ", a)
    a <- gsub("[a-z]*mmm[a-z]*", " ", a)
    a <- gsub("[a-z]*nnn[a-z]*", " ", a)
    a <- gsub("[a-z]*ooo[a-z]*", " ", a)
    a <- gsub("[a-z]*ppp[a-z]*", " ", a)
    a <- gsub("[a-z]*qqq[a-z]*", " ", a)
    a <- gsub("[a-z]*rrr[a-z]*", " ", a)
    a <- gsub("[a-z]*sss[a-z]*", " ", a)
    a <- gsub("[a-z]*ttt[a-z]*", " ", a)
    a <- gsub("[a-z]*uuu[a-z]*", " ", a)
    a <- gsub("[a-z]*vvv[a-z]*", " ", a)
    a <- gsub("[a-z]*www[a-z]*", " ", a)
    a <- gsub("[a-z]*xxx[a-z]*", " ", a)
    a <- gsub("[a-z]*yyy[a-z]*", " ", a)
    gsub("[a-z]*zzz[a-z]*", " ", a)
}




####Get a sample of all the three datasets##########
set.seed(12345)
blogSamp <- sample_files(blogs)#using the sample_files function defined above to sample 10% of the file
newsSamp <- sample_files(news)
twitSamp <- sample_files(twit)
rm(blogs,news,twit)##removes the original datasets to free up memory

##combine all three sampled files and remove the individual sample files

allSamp<-c(blogSamp,newsSamp,twitSamp)
rm(blogSamp)
rm(newsSamp)
rm(twitSamp)

write.table(allSamp, file="C:\\Users\\smsanda\\Documents\\en_US\\allSamples.txt", row.names = F)

library(tm)

#####Create a corpus#########################################################################
allSamp<-VCorpus(DirSource(directory="C:\\Users\\smsanda\\Documents\\en_US",encoding="UTF-8"),readerControl = list(language="us"))

###Clean the combined file

allSamp<- tm_map(allSamp, removeNumbers)
allSamp <- tm_map(allSamp, content_transformer(tolower))
allSamp <- tm_map(allSamp, content_transformer(detectNonAsciiChar))
allSamp <- tm_map(allSamp, content_transformer(removeNonAsciiWord))
allSamp<- tm_map(allSamp, content_transformer(removeHTTPS))
allSamp <- tm_map(allSamp, content_transformer(removeHTTP))
allSamp <- tm_map(allSamp, content_transformer(removeFTP))
allSamp <- tm_map(allSamp, content_transformer(removeWWW))
allSamp <- tm_map(allSamp, content_transformer(removeHashTag))
allSamp <- tm_map(allSamp, content_transformer(removeTwitterRT))
allSamp<- tm_map(allSamp, content_transformer(removeCharRepetition))
allSamp <- tm_map(allSamp, removePunctuation)
allSamp <- tm_map(allSamp , stripWhitespace)
allSamp <- tm_map(allSamp, removeWords, stopwords("english"))
allSamp <- tm_map(allSamp, stemDocument)

##Load list of profanity
badWordsURL<-"https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
download.file(badWordsURL,destfile="badWords.txt")
badWords<-read.csv("badWords.txt",header=FALSE,sep="\n",strip.white=TRUE)
allSamp<-tm_map(allSamp,removeWords,badWords[,1])



#Word/phrase count function
freqDataFrame <- function(tdm){
  # Helper function to tabulate frequency
  freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
  freqDataFrame <- data.frame(word=names(freq), freq=freq)
  return(freqDataFrame)
}


#n-gram Functions
library(RWeka)
library(ggplot2)
singleFunc<-function(x) unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
biFunc<-  function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
triFunc<-  function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
quadFunc<-  function(x) unlist(lapply(ngrams(words(x), 4), paste, collapse = " "), use.names = FALSE)
pentaFunc<-  function(x) unlist(lapply(ngrams(words(x), 5), paste, collapse = " "), use.names = FALSE)


#Constructing the nGrams
#unigram words
unigram<- TermDocumentMatrix(allSamp, control=list(tokenize=unigramFunc))
unigram <- removeSparseTerms(unigram, 0.99)
unigramfreq <- freqDataFrame(unigram)
unigramfreq$word <- factor(unigramfreq$word, levels = unigramfreq$word[order(-unigramfreq$freq)])

#bigrams
bigram<- TermDocumentMatrix(allSamp, control=list(tokenize=biFunc))
bigram <- removeSparseTerms(bigram, 0.999)
bigramfreq <- freqDataFrame(bigram)
bigramfreq$word <- factor(bigramfreq$word, levels = biGramfreq$word[order(-bigramfreq$freq)])

#trigrams
trigram<- TermDocumentMatrix(allSamp, control=list(tokenize=triFunc))
trigram <- removeSparseTerms(trigram, 0.999)
trigramfreq <- freqDataFrame(trigram)
trigramfreq$word <- factor(trigramfreq$word, levels = trigramfreq$word[order(-trigramfreq$freq)])

#quadgrams
quadgram<- TermDocumentMatrix(allSamp, control=list(tokenize=quadFunc))
quadgram <- removeSparseTerms(quadgram, 0.9999)
quadgramfreq <- freqDataFrame(quadgram)
quadgramfreq$word <- factor(quadgramfreq$word, levels = quadgramfreq$word[order(-quadgramfreq$freq)])

#pentagrams
pentagram<- TermDocumentMatrix(allSamp, control=list(tokenize=pentaFunc))
pentagram <- removeSparseTerms(pentagram, 0.9999)
pentagramfreq <- freqDataFrame(pentagram)
pentagramfreq$word <- factor(pentagramfreq$word, levels = quadgramfreq$word[order(-quadgramfreq$freq)])

########################Write the frequency datasets to file; to be used later for app upload#########################

write.csv(freq_uni, file="unigrams.csv", row.names = F)
write.csv(freq_bi, file="bigrams.csv", row.names = F)
write.csv(freq_tri, file="trigrams.csv", row.names = F)
write.csv(freq_quadi, file="quadragrams.csv", row.names = F)
write.csv(freq_penta, file="pentagrams.csv", row.names = F)
